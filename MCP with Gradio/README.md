# ğŸŒ¦ MCP Weather Assistant (Gradio + FastMCP + GPT-4o-mini)

This project demonstrates how to build a **Model Context Protocol (MCP)** application using:

* **FastMCP** as a tool server
* **OpenAI GPT-4o-mini** as the LLM client
* **Gradio** as a web UI
* **stdio-based MCP transport** (no HTTP server needed)

The assistant can answer weather-related questions by **dynamically calling MCP tools** exposed by a FastMCP server.

---

## âœ¨ Features

* âœ… Model Context Protocol (MCP) compliant
* ğŸ”§ Tool calling via FastMCP
* ğŸ¤– LLM orchestration using GPT-4o-mini
* ğŸ–¥ Interactive Gradio chatbot UI
* âš¡ Async, non-blocking architecture
* ğŸ§© Clean separation: **Server / Client / UI**

---

## ğŸ§  Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Gradio UI      â”‚
â”‚ (Chatbot frontend) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ user query
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MCP Gradio Client â”‚
â”‚  (client.py logic)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ 1. Ask LLM (GPT-4o-mini)
          â”‚ 2. Decide tool call
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OpenAI API       â”‚
â”‚ (GPT-4o-mini)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ function_call
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MCP ClientSession â”‚
â”‚ (stdio transport)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FastMCP Server     â”‚
â”‚ (server.py)        â”‚
â”‚ Weather tools      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Idea

* The **LLM never fetches weather directly**
* It **decides** whether to call a tool
* MCP executes the tool
* Results are returned to the LLM
* The LLM produces the final answer

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ server.py        # FastMCP tool server (weather tools)
â”œâ”€â”€ mcp_gradio_app.py           # Gradio + MCP client
â”œâ”€â”€ .env             # OpenAI API key
â”œâ”€â”€ pyproject.toml   # uv / Python dependencies
â”œâ”€â”€ assets   # uv / Python dependencies
â””â”€â”€ README.md
```

---

## ğŸ”§ Prerequisites

### 1. Python

* Python **3.10+** recommended

Check:

```bash
python --version
```

---

### 2. uv (recommended)

This project uses **uv** for fast dependency management.

Install uv:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

Or via pip:

```bash
pip install uv
```

---

### 3. OpenAI API Key

Create a `.env` file:

```env
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx
```

---

## ğŸ“¦ Dependencies

Main libraries used:

* `mcp`
* `gradio`
* `openai`
* `httpx`
* `python-dotenv`

Install everything:

```bash
uv add mcp gradio openai httpx python-dotenv
```

or (if not using uv):

```bash
pip install -r requirements.txt
```

---

## ğŸš€ How to Run

### 1ï¸âƒ£ Start the Gradio app (auto-starts MCP server)

```bash
python app.py
```

You should see:

```
Connected MCP tools: ['get_alerts', 'get_forecast']
Running on http://127.0.0.1:7860
```

Open your browser at:

ğŸ‘‰ **[http://localhost:7860](http://localhost:7860)**

---

### 2ï¸âƒ£ Ask questions like

* `What are the weather alerts in Washington?`
* `Get forecast for latitude 40.7 longitude -74.0`
* `Is there any severe weather in California?`

The LLM will automatically decide when to call MCP tools.

---

## ğŸ” How Tool Calling Works (Important)

1. User enters a question
2. GPT-4o-mini receives:

   * user query
   * MCP tool schemas
3. GPT chooses:

   * âŒ answer directly
   * âœ… or call a tool (e.g. `get_alerts`)
4. MCP executes the tool
5. Tool result is fed back to GPT
6. GPT produces final response

âœ” Fully aligned with MCP design philosophy.

---

## ğŸ§ª Debugging Tips

### UI stuck on â€œprocessingâ€¦â€

* Ensure:

  * MCP server started correctly
  * You are using **AsyncOpenAI**
  * No blocking calls inside Gradio handlers

### Gradio error about message format

Make sure Chatbot uses:

```python
gr.Chatbot(type="messages")
```

And messages look like:

```python
{"role": "user", "content": "..."}
{"role": "assistant", "content": "..."}
```

---

## Reference

https://modelcontextprotocol.io/docs
